{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50399205ee0cfdde1957058237c765cf807d4925"
   },
   "source": [
    "### Motivation\n",
    "I'm working on a larger project where I need to be able to count syllables and find rhyming words for words that don't appear in the dictionary. (Think proper-nouns and made up words.) Being able to predict how words are pronounced is going to be useful for both tasks. I thought it'd be nice to document my soultion and process. \n",
    "\n",
    "I'm hoping this will be useful for beginners and people with more experience. I've done my best to explain my choices while providing links to background info. I wanted the code to be easy to follow so in a few cases brevity and efficiency are sacrificed for readability. Everything is done in python3 & Keras. Let's get started!\n",
    "\n",
    "### Contents\n",
    "1. [Dataset](#Dataset)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "3. [Baseline Model](#Baseline-Model)\n",
    "     - Training\n",
    "     - Prediction\n",
    "     - Evaluation\n",
    "4. [Character & Phoneme Embeddings](#Character-&-Phoneme-Embeddings)\n",
    "    - Training\n",
    "    - Evaluation\n",
    "    - Visualizing the Embeddings\n",
    "5. [Bidirectional Encoder & Attention Mechanism](#Bidirectional-Encoder-&-Attention-Decoder)\n",
    "    - Training\n",
    "    - Evaluation\n",
    "    - Final Training & Evaluation\n",
    "6. [Beamsearch](#Beamsearch)\n",
    "7. [Results](#Results)\n",
    "    - Sampling Incorrect Predictions\n",
    "    - Ideas for Further Improvement\n",
    "    - Advanced Techniques\n",
    "    - Acknowledgements\n",
    "\n",
    "# Dataset\n",
    "We'll use the [CMU Pronunciation Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict) which maps 134,000 words to their phonetic spellings. For example, \"apple\" appears in the dictionary as: \"AE1 P AH0 L\". Each token, without numbers (AE, P, AH, etc.), represents a sound and is called \"phoneme.\" The numbers on the end of some phonemes represent how much ephasis the sound gets. These numbers are  referred to as \"lexical stress markers.\" There are 39 unique phonemes and 84 unique symbols since only vowel sounds have stress markers.\n",
    "\n",
    "Let's load the dictionary and do a little data cleaning while we're at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# To make sure our kernel runs all the way through and gets saved,\n",
    "# we'll trim some things back and skip training\n",
    "LOAD_MODELS = True \n",
    "\n",
    "CMU_DICT_PATH = os.path.join(\n",
    "    './', 'cmu-pronunciation-dictionary-unmodified-07b', 'cmudict-0.7b')\n",
    "CMU_SYMBOLS_PATH = os.path.join(\n",
    "    './', 'cmu-pronouncing-dictionary', 'cmudict.symbols')\n",
    "\n",
    "# Skip words with numbers or symbols\n",
    "ILLEGAL_CHAR_REGEX = \"[^A-Z-'.]\"\n",
    "\n",
    "# Only 3 words are longer than 20 chars\n",
    "# Setting a limit now simplifies training our model later\n",
    "MAX_DICT_WORD_LEN = 20\n",
    "MIN_DICT_WORD_LEN = 2\n",
    "\n",
    "\n",
    "def load_clean_phonetic_dictionary():\n",
    "\n",
    "    def is_alternate_pho_spelling(word):\n",
    "        # No word has > 9 alternate pronounciations so this is safe\n",
    "        return word[-1] == ')' and word[-3] == '(' and word[-2].isdigit() \n",
    "\n",
    "    def should_skip(word):\n",
    "        if not word[0].isalpha():  # skip symbols\n",
    "            return True\n",
    "        if word[-1] == '.':  # skip abbreviations\n",
    "            return True\n",
    "        if re.search(ILLEGAL_CHAR_REGEX, word):\n",
    "            return True\n",
    "        if len(word) > MAX_DICT_WORD_LEN:\n",
    "            return True\n",
    "        if len(word) < MIN_DICT_WORD_LEN:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    phonetic_dict = {}\n",
    "    with open(CMU_DICT_PATH, encoding=\"ISO-8859-1\") as cmu_dict:\n",
    "        for line in cmu_dict:\n",
    "\n",
    "            # Skip commented lines\n",
    "            if line[0:3] == ';;;':\n",
    "                continue\n",
    "\n",
    "            word, phonetic = line.strip().split('  ')\n",
    "\n",
    "            # Alternate pronounciations are formatted: \"WORD(#)  F AH0 N EH1 T IH0 K\"\n",
    "            # We don't want to the \"(#)\" considered as part of the word\n",
    "            if is_alternate_pho_spelling(word):\n",
    "                word = word[:word.find('(')]\n",
    "\n",
    "            if should_skip(word):\n",
    "                continue\n",
    "\n",
    "            if word not in phonetic_dict:\n",
    "                phonetic_dict[word] = []\n",
    "            phonetic_dict[word].append(phonetic)\n",
    "\n",
    "    if LOAD_MODELS: # limit dataset to 5,000 words\n",
    "        phonetic_dict = {key:phonetic_dict[key] \n",
    "                         for key in random.sample(list(phonetic_dict.keys()), 5000)}\n",
    "    return phonetic_dict\n",
    "\n",
    "phonetic_dict = load_clean_phonetic_dictionary()\n",
    "example_count = np.sum([len(prons) for _, prons in phonetic_dict.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62aced3c8805c744c5782a7de61de9d1f9c5049e"
   },
   "source": [
    "Let's take a peek at our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JEFFRIES --> JH EH1 F R IY0 Z\n",
      "TRISTRAM --> T R IH1 S T R AH0 M\n",
      "BRZEZINSKI --> B R IH0 Z IH1 N S K IY0\n",
      "HOROVITZ --> HH AA1 R AH0 V IH0 T S\n",
      "PUNCHING --> P AH1 N CH IH0 NG\n",
      "KROUSE --> K R AW1 S\n",
      "FLICKS --> F L IH1 K S\n",
      "DEHERRERA --> D EY0 HH EH0 R EH1 R AH0\n",
      "LOWMAN --> L OW1 M AH0 N\n",
      "JETSONS --> JH EH1 T S AH0 N Z\n",
      "\n",
      "After cleaning, the dictionary contains 5000 words and 5333 pronunciations (333 are alternate pronunciations).\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([k+' --> '+phonetic_dict[k][0] for k in random.sample(list(phonetic_dict.keys()), 10)]))\n",
    "print('\\nAfter cleaning, the dictionary contains %s words and %s pronunciations (%s are alternate pronunciations).' % \n",
    "      (len(phonetic_dict), example_count, (example_count-len(phonetic_dict))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01120130799a96aa43d169ceafc5ce9865663525"
   },
   "source": [
    "# Data Preparation\n",
    "Next, before we can do any learning, we need to come up with a way to numerically represent words and pronunciations. We'll treat words as squences of characters and pronunciations as sequences of phoneme symbols (including lexical stress markers). We can assign each character and each phoneme a number. Later we'll use these numbers to represent chars/phonemes as 1-hot vectors. Predicting phonemes from a word's letters is sometimes referred to as grapheme-to-phoneme conversion.\n",
    "\n",
    "We'll need to tell our model where a phonetic spelling starts and ends so we'll introduce 2 special start & end symbols, arbitrarily represented by the tab and newline characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "132e94f6b03eaaf77237e18ce0675db3ab2d1f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char to id mapping: \n",
      " {'': 0, '.': 1, '-': 2, \"'\": 3, 'A': 4, 'B': 5, 'C': 6, 'D': 7, 'E': 8, 'F': 9, 'G': 10, 'H': 11, 'I': 12, 'J': 13, 'K': 14, 'L': 15, 'M': 16, 'N': 17, 'O': 18, 'P': 19, 'Q': 20, 'R': 21, 'S': 22, 'T': 23, 'U': 24, 'V': 25, 'W': 26, 'X': 27, 'Y': 28, 'Z': 29}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "START_PHONE_SYM = '\\t'\n",
    "END_PHONE_SYM = '\\n'\n",
    "\n",
    "\n",
    "def char_list():\n",
    "    allowed_symbols = [\".\", \"-\", \"'\"]\n",
    "    uppercase_letters = list(string.ascii_uppercase)\n",
    "    return [''] + allowed_symbols + uppercase_letters\n",
    "\n",
    "\n",
    "def phone_list():\n",
    "    phone_list = [START_PHONE_SYM, END_PHONE_SYM]\n",
    "    with open(CMU_SYMBOLS_PATH) as file:\n",
    "        for line in file: \n",
    "            phone_list.append(line.strip())\n",
    "    return [''] + phone_list\n",
    "\n",
    "\n",
    "def id_mappings_from_list(str_list):\n",
    "    str_to_id = {s: i for i, s in enumerate(str_list)} \n",
    "    id_to_str = {i: s for i, s in enumerate(str_list)}\n",
    "    return str_to_id, id_to_str\n",
    "\n",
    "\n",
    "# Create character to ID mappings\n",
    "char_to_id, id_to_char = id_mappings_from_list(char_list())\n",
    "\n",
    "# Load phonetic symbols and create ID mappings\n",
    "phone_to_id, id_to_phone = id_mappings_from_list(phone_list())\n",
    "\n",
    "# Example:\n",
    "print('Char to id mapping: \\n', char_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "311046c02b01ba12ef7e646b0f5426da62504220"
   },
   "source": [
    "It might be tempting to just use these id's as inputs for our model. But doing so implies a relationship between the letters/phonemes that doesn't really exist. For example, since A=4, C=6, and U=24 the implication is that A and C are somehow more alike than A and U (since 4 is closer to 6). This is obviously not the case. Instead, we can use our id mappings to convert chars and phonemes to [1-hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "5436059c760f6065eae0165922e5ea41f1f2f44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A\" is represented by:\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.] \n",
      "-----\n",
      "\"AH0\" is represented by:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "CHAR_TOKEN_COUNT = len(char_to_id)\n",
    "PHONE_TOKEN_COUNT = len(phone_to_id)\n",
    "\n",
    "\n",
    "def char_to_1_hot(char):\n",
    "    char_id = char_to_id[char]\n",
    "    hot_vec = np.zeros((CHAR_TOKEN_COUNT))\n",
    "    hot_vec[char_id] = 1.\n",
    "    return hot_vec\n",
    "\n",
    "\n",
    "def phone_to_1_hot(phone):\n",
    "    phone_id = phone_to_id[phone]\n",
    "    hot_vec = np.zeros((PHONE_TOKEN_COUNT))\n",
    "    hot_vec[phone_id] = 1.\n",
    "    return hot_vec\n",
    "\n",
    "# Example:\n",
    "print('\"A\" is represented by:\\n', char_to_1_hot('A'), '\\n-----')\n",
    "print('\"AH0\" is represented by:\\n', phone_to_1_hot('AH0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cddc5cbb7f69bd855ccfb470705c6ec7f77b9538"
   },
   "source": [
    "Now that we have a way to numerically represent letters and sounds (phonemes), we can convert our entire dataset into two, big 3D matricies (tensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "636753d1db96975b780332c40a106e79799beafb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Matrix Shape:  (5333, 18, 30)\n",
      "Pronunciation Matrix Shape:  (5333, 19, 87)\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_SEQ_LEN = max([len(word) for word, _ in phonetic_dict.items()])\n",
    "MAX_PHONE_SEQ_LEN = max([max([len(pron.split()) for pron in pronuns]) \n",
    "                         for _, pronuns in phonetic_dict.items()]\n",
    "                       ) + 2  # + 2 to account for the start & end tokens we need to add\n",
    "\n",
    "\n",
    "def dataset_to_1_hot_tensors():\n",
    "    char_seqs = []\n",
    "    phone_seqs = []\n",
    "    \n",
    "    for word, pronuns in phonetic_dict.items():\n",
    "        word_matrix = np.zeros((MAX_CHAR_SEQ_LEN, CHAR_TOKEN_COUNT))\n",
    "        for t, char in enumerate(word):\n",
    "            word_matrix[t, :] = char_to_1_hot(char)\n",
    "        for pronun in pronuns:\n",
    "            pronun_matrix = np.zeros((MAX_PHONE_SEQ_LEN, PHONE_TOKEN_COUNT))\n",
    "            phones = [START_PHONE_SYM] + pronun.split() + [END_PHONE_SYM]\n",
    "            for t, phone in enumerate(phones):\n",
    "                pronun_matrix[t,:] = phone_to_1_hot(phone)\n",
    "                \n",
    "            char_seqs.append(word_matrix)\n",
    "            phone_seqs.append(pronun_matrix)\n",
    "    \n",
    "    return np.array(char_seqs), np.array(phone_seqs)\n",
    "            \n",
    "\n",
    "char_seq_matrix, phone_seq_matrix = dataset_to_1_hot_tensors()        \n",
    "print('Word Matrix Shape: ', char_seq_matrix.shape)\n",
    "print('Pronunciation Matrix Shape: ', phone_seq_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9745b486a3f7e1feb7f141b9c351a45759119cf9"
   },
   "source": [
    "# Baseline Model\n",
    "\n",
    "Since we're dealing with sequence data, an **RNN**[[video](https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model),[blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)] a good choice. Let's start with an **LSTM**[[video](https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm),[blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)] based RNN.\n",
    "\n",
    "Notice that the number of characters in a word is often not the same as the number of phonemes in a pronunciation. There's no 1-to-1 mapping between our inputs and outputs. For that reason, we'll create a **sequence to sequence**[[blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)] model with 2 parts, an encoder and a decoder. \n",
    "\n",
    "We'll feed our word representations to the encoder one character at a time and then pass the encoder's state variables to the decoder. We need slightly different decoder setups for training vs. test time. During training, we'll feed the decoder the correct pronunciations, one phoneme at a time. At each timestep, the decoder will predict the next phoneme. During inference (test time), we don't know the correct phoneme sequence (at least in theory). So we'll feed the decoder's output from the previous time step into the next timestep as input. This is why we needed the `START_PHONE_SYM` mentioned earlier. Here's an illustration of how our network will work at test time: \n",
    "\n",
    "![](https://i.imgur.com/vzkiAnZ.png)\n",
    "\n",
    "The `phone_seq_matrix` we created above will be the input to our decoder. We'll create the decoder output by shifting all the phone sequences to the left by 1 step. The decoder outputs won't contain the start token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b035166910acd2597a5a8e22ad504fbabbf5c12e"
   },
   "outputs": [],
   "source": [
    "phone_seq_matrix_decoder_output = np.pad(phone_seq_matrix,((0,0),(0,1),(0,0)), mode='constant')[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "035480bf8bc19e150e1932534f11798046b6e931"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "def baseline_model(hidden_nodes = 256):\n",
    "    \n",
    "    # Shared Components - Encoder\n",
    "    char_inputs = Input(shape=(None, CHAR_TOKEN_COUNT))\n",
    "    encoder = LSTM(hidden_nodes, return_state=True)\n",
    "    \n",
    "    # Shared Components - Decoder\n",
    "    phone_inputs = Input(shape=(None, PHONE_TOKEN_COUNT))\n",
    "    decoder = LSTM(hidden_nodes, return_sequences=True, return_state=True)\n",
    "    decoder_dense = Dense(PHONE_TOKEN_COUNT, activation='softmax')\n",
    "    \n",
    "    # Training Model\n",
    "    _, state_h, state_c = encoder(char_inputs) # notice encoder outputs are ignored\n",
    "    encoder_states = [state_h, state_c]\n",
    "    decoder_outputs, _, _ = decoder(phone_inputs, initial_state=encoder_states)\n",
    "    phone_prediction = decoder_dense(decoder_outputs)\n",
    "\n",
    "    training_model = Model([char_inputs, phone_inputs], phone_prediction)\n",
    "    \n",
    "    # Testing Model - Encoder\n",
    "    testing_encoder_model = Model(char_inputs, encoder_states)\n",
    "    \n",
    "    # Testing Model - Decoder\n",
    "    decoder_state_input_h = Input(shape=(hidden_nodes,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_nodes,))\n",
    "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder(phone_inputs, initial_state=decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    phone_prediction = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    testing_decoder_model = Model([phone_inputs] + decoder_state_inputs, [phone_prediction] + decoder_states)\n",
    "    \n",
    "    return training_model, testing_encoder_model, testing_decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f922704ad733636024b0bbdd5ff3fbeb811848de"
   },
   "source": [
    "## Training\n",
    "First, we'll split off a test set so we can get a fair evaluation of our model's performance later. For demonstration, we'll cut the test size down to just 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0a1babcb42f8314cbfcccf366bdc425740cf5d4a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "    \n",
    "(char_input_train, char_input_test, \n",
    " phone_input_train, phone_input_test, \n",
    " phone_output_train, phone_output_test) = train_test_split(\n",
    "    char_seq_matrix, phone_seq_matrix, phone_seq_matrix_decoder_output, \n",
    "    test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "TEST_EXAMPLE_COUNT = char_input_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db8c2a195cc644926b23f110612249af2f87b632",
    "collapsed": true
   },
   "source": [
    "Now we'll train our sequence to sequence model until it starts to overfit. We want a model that generalizes well to previously unseen examples so we'll keep the version that has the lowest validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a5bd50fb6fb2053fe30a7cbc11e8117c985f7b07"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def train(model, weights_path, encoder_input, decoder_input, decoder_output):\n",
    "    checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, save_best_only=True)\n",
    "    stopper = EarlyStopping(monitor='val_loss',patience=3)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit([encoder_input, decoder_input], decoder_output,\n",
    "          batch_size=256,\n",
    "          epochs=100,\n",
    "          validation_split=0.2, # Keras will automatically create a validation set for us\n",
    "          callbacks=[checkpointer, stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "b7597b6a335784d17031657ca5cfd664d90c6135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tarun/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "BASELINE_MODEL_WEIGHTS = os.path.join(\n",
    "    './', 'predicting-english-pronunciations-model-weights', 'baseline_model_weights.hdf5')\n",
    "training_model, testing_encoder_model, testing_decoder_model = baseline_model()\n",
    "if not LOAD_MODELS:\n",
    "    train(training_model, BASELINE_MODEL_WEIGHTS, char_input_train, phone_input_train, phone_output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f31495e07469d4c6c61552657645de96102cb053"
   },
   "source": [
    "## Prediction\n",
    "During training, at each time step, we fed our decoder the correct ouput from the previous time time step. As previously mentioned, we don't know what the *correct* ouput is at test time, only what the decoder predicted. So we need a different procedure to make predictions:\n",
    "1. Encode the input word (char sequence) as state vectors using the encoder model.\n",
    "2. Pass the encoder's state variables to the decoder. \n",
    "3. Feed the start token to the decoder to get a phoneme prediction at the first time step.\n",
    "4. Pass the updated states and 1st phoneme prediction as input to the decoder to get the 2nd predicted phoneme.\n",
    "5. Pass the updated states and 2nd phoneme into the decoder to get the 3rd phoneme and so on until the decoder predicts a stop token or we hit the maximum pronunciation length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5e5313319d19229a3275ffe42066616e6b7e2efc"
   },
   "outputs": [],
   "source": [
    "def predict_baseline(input_char_seq, encoder, decoder):\n",
    "    state_vectors = encoder.predict(input_char_seq) \n",
    "    \n",
    "    prev_phone = np.zeros((1, 1, PHONE_TOKEN_COUNT))\n",
    "    prev_phone[0, 0, phone_to_id[START_PHONE_SYM]] = 1.\n",
    "    \n",
    "    end_found = False \n",
    "    pronunciation = '' \n",
    "    while not end_found:\n",
    "        decoder_output, h, c = decoder.predict([prev_phone] + state_vectors)\n",
    "        \n",
    "        # Predict the phoneme with the highest probability\n",
    "        predicted_phone_idx = np.argmax(decoder_output[0, -1, :])\n",
    "        predicted_phone = id_to_phone[predicted_phone_idx]\n",
    "        \n",
    "        pronunciation += predicted_phone + ' '\n",
    "        \n",
    "        if predicted_phone == END_PHONE_SYM or len(pronunciation.split()) > MAX_PHONE_SEQ_LEN: \n",
    "            end_found = True\n",
    "        \n",
    "        # Setup inputs for next time step\n",
    "        prev_phone = np.zeros((1, 1, PHONE_TOKEN_COUNT))\n",
    "        prev_phone[0, 0, predicted_phone_idx] = 1.\n",
    "        state_vectors = [h, c]\n",
    "        \n",
    "    return pronunciation.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34759c3f7d8268a8df66fd0f9bd70de83cd19132"
   },
   "source": [
    "Let's do a quick manual sanity check to see how our model is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "640114347d815fb63e58a55239148575cfc4b631"
   },
   "outputs": [],
   "source": [
    "# Helper method for converting vector representations back into words\n",
    "def one_hot_matrix_to_word(char_seq):\n",
    "    word = ''\n",
    "    for char_vec in char_seq[0]:\n",
    "        if np.count_nonzero(char_vec) == 0:\n",
    "            break\n",
    "        hot_bit_idx = np.argmax(char_vec)\n",
    "        char = id_to_char[hot_bit_idx]\n",
    "        word += char\n",
    "    return word\n",
    "\n",
    "\n",
    "# Some words have multiple correct pronunciations\n",
    "# If a prediction matches any correct pronunciation, consider it correct.\n",
    "def is_correct(word,test_pronunciation):\n",
    "    correct_pronuns = phonetic_dict[word]\n",
    "    for correct_pronun in correct_pronuns:\n",
    "        if test_pronunciation == correct_pronun:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def sample_baseline_predictions(sample_count, word_decoder):\n",
    "    sample_indices = random.sample(range(TEST_EXAMPLE_COUNT), sample_count)\n",
    "    for example_idx in sample_indices:\n",
    "        example_char_seq = char_input_test[example_idx:example_idx+1]\n",
    "        predicted_pronun = predict_baseline(example_char_seq, testing_encoder_model, testing_decoder_model)\n",
    "        example_word = word_decoder(example_char_seq)\n",
    "        pred_is_correct = is_correct(example_word, predicted_pronun)\n",
    "        print('✅ ' if pred_is_correct else '❌ ', example_word,'-->', predicted_pronun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "7062efa18b14ae22cdefb152e0e13d58917c8c29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌  RECOGNISE --> R IY0 K AO1 G N AY2 Z\n",
      "✅  SEASE --> S IY1 Z\n",
      "❌  BUESCHER --> B UW1 SH ER0\n",
      "✅  SCHMOKE --> SH M OW1 K\n",
      "✅  ALTUS --> AE1 L T AH0 S\n",
      "❌  ZAVODNY --> Z AA0 V AA1 D N IY0\n",
      "❌  MALEFACTORS --> M AE2 L AH0 F AE1 K T ER0 Z\n",
      "❌  MANSEAU --> M AE1 N S UW2\n",
      "❌  JAMIESON --> JH EY1 M IY0 AH0 S\n",
      "✅  KIDDE --> K IH1 D\n",
      "❌  KANAN --> K AE1 N AH0 N\n",
      "✅  ENLISTING --> EH0 N L IH1 S T IH0 NG\n",
      "✅  COOVER --> K UW1 V ER0\n",
      "❌  GUADAGNO --> G W AA2 D AA1 G AH0 N\n",
      "❌  SCISSOR --> S IH1 S ER0\n",
      "✅  EYESHADE --> AY1 SH EY2 D\n",
      "✅  BLOWN --> B L OW1 N\n",
      "❌  EKATERINA --> EH2 K AH0 T IH1 R IY0 AH0 N\n",
      "✅  CARICO --> K AA0 R IY1 K OW0\n",
      "❌  MCADOW --> M AH0 K D AW1\n",
      "✅  BABBLING --> B AE1 B AH0 L IH0 NG\n",
      "✅  SLINGERLAND --> S L IH1 NG G ER0 L AH0 N D\n",
      "✅  DIPRIMA --> D IH0 P R IY1 M AH0\n",
      "❌  KENNEBREW --> K EH1 N AH0 B R UW2\n",
      "❌  AUDITORY --> AO1 D AH0 T AO2 R IY0\n",
      "❌  CARPORTS --> K AA1 R P ER0 T S\n",
      "✅  COLM --> K OW1 L M\n",
      "✅  NASHBURG --> N AE1 SH B ER0 G\n",
      "✅  KINGTON --> K IH1 NG T AH0 N\n",
      "✅  SINGLES --> S IH1 NG G AH0 L Z\n"
     ]
    }
   ],
   "source": [
    "training_model.load_weights(BASELINE_MODEL_WEIGHTS)  # also loads weights for testing models\n",
    "sample_baseline_predictions(30, one_hot_matrix_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e3c452bc62a7a843cbf01e32850875af391d540"
   },
   "source": [
    "These all look pretty good. Even the incorrect predictions seem reasonable.\n",
    "\n",
    "## Evaluation\n",
    "We'll use 3 different metrics to evaluate our model. \n",
    "\n",
    "**1. Syllable Count Accuracy**: remember one of the original goals of this project was to be able to count the number of syllables for words not found in the dictionary. Getting a syllable count from a phonetic spelling is as easy as counting the phonemes with stress markers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "5d95b69e675cfcafc4de389a8e2b1df8276cf878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NONLINEAR -- 4 syllables\n",
      "MOONIE -- 2 syllables\n",
      "KUHNLE -- 2 syllables\n"
     ]
    }
   ],
   "source": [
    "def syllable_count(phonetic_sp): \n",
    "    count = 0\n",
    "    for phone in phonetic_sp.split(): \n",
    "        if phone[-1].isdigit():\n",
    "            count += 1 \n",
    "    return count\n",
    "\n",
    "# Examples:\n",
    "for ex_word in list(phonetic_dict.keys())[:3]:\n",
    "    print(ex_word, '--', syllable_count(phonetic_dict[ex_word][0]), 'syllables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b34b3daefc098ad485cafdddd7ce7d317eb4f52"
   },
   "source": [
    "**2. Perfect Accuracy**: % of test examples where every predicted phoneme and stress marker is correct and in the correct order. \n",
    "\n",
    "**3. Average Bleu Score:** will give our model credit for predictions that are close. A perfect prediction is a 1.0 and a total mismatch is a 0.0. This metric is often used to evaluate language translation, which, if you think about it, is pretty similar to pronunciation prediction. Read more about it [here](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3406df17756ec7b8266c39a84e8e2a8ee0d107cd"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def is_syllable_count_correct(word, test_pronunciation):\n",
    "    correct_pronuns = phonetic_dict[word]\n",
    "    for correct_pronun in correct_pronuns:\n",
    "        if syllable_count(test_pronunciation) == syllable_count(correct_pronun):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def bleu_score(word,test_pronunciation):\n",
    "    references = [pronun.split() for pronun in phonetic_dict[word]]\n",
    "    smooth = SmoothingFunction().method1\n",
    "    return sentence_bleu(references, test_pronunciation.split(), smoothing_function=smooth)\n",
    "\n",
    "\n",
    "def evaluate(test_examples, encoder, decoder, word_decoder, predictor):\n",
    "    correct_syllable_counts = 0\n",
    "    perfect_predictions = 0\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for example_idx in range(TEST_EXAMPLE_COUNT):\n",
    "        example_char_seq = test_examples[example_idx:example_idx+1]\n",
    "        predicted_pronun = predictor(example_char_seq, encoder, decoder)\n",
    "        example_word = word_decoder(example_char_seq)\n",
    "        \n",
    "        perfect_predictions += is_correct(example_word,predicted_pronun)\n",
    "        correct_syllable_counts += is_syllable_count_correct(example_word,predicted_pronun)\n",
    "\n",
    "        bleu = bleu_score(example_word,predicted_pronun)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "    syllable_acc = correct_syllable_counts / TEST_EXAMPLE_COUNT\n",
    "    perfect_acc = perfect_predictions / TEST_EXAMPLE_COUNT\n",
    "    avg_bleu_score = np.mean(bleu_scores)\n",
    "    \n",
    "    return syllable_acc, perfect_acc, avg_bleu_score\n",
    "\n",
    "\n",
    "def print_results(model_name, syllable_acc, perfect_acc, avg_bleu_score):\n",
    "    print(model_name)\n",
    "    print('-'*20)\n",
    "    print('Syllable Accuracy: %s%%' % round(syllable_acc*100, 1))\n",
    "    print('Perfect Accuracy: %s%%' % round(perfect_acc*100, 1))\n",
    "    print('Bleu Score: %s' % round(avg_bleu_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "712d73c3fb49e3969dd9757ddd4ab3f4e0b75047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model\n",
      "--------------------\n",
      "Syllable Accuracy: 93.9%\n",
      "Perfect Accuracy: 57.0%\n",
      "Bleu Score: 0.6937\n"
     ]
    }
   ],
   "source": [
    "syllable_acc, perfect_acc, avg_bleu_score = evaluate(\n",
    "    char_input_test, testing_encoder_model, testing_decoder_model, one_hot_matrix_to_word, predict_baseline)\n",
    "print_results('Baseline Model',syllable_acc, perfect_acc, avg_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e8b54c377afd7cc5a6cd584bd0b0d48083b52a3"
   },
   "source": [
    "Great! Our scores so far are decent. Let's see if we can find a few ways to improve on our baseline model. \n",
    "\n",
    "Before we do, let's free up some memory by removing our baseline model from the Tensorflow graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "6e7451096046032e7296e12f229301c54c4b69d0"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e2bfd192e809282e736ddf8cd2df248b744e819"
   },
   "source": [
    "# Character & Phoneme Embeddings\n",
    "Instead of representing letters and phonemes as 1-hot vectors, we can use an **embedding**[[video](https://www.coursera.org/learn/nlp-sequence-models/lecture/K604Z/embedding-matrix),[blog](https://www.quora.com/What-is-word-embedding-in-deep-learning)] so our model will learn it's own representations of each symbol. Embeddings are more descriptive representations than 1-hot vectors. Think about how the letter 'c' sometimes sounds like a 'k' and other times sounds like an 's'. In theory, our embedding layer should learn these kinds of relationships. Hopefully this will help improve our scores.\n",
    "\n",
    "Keras's `Embedding` layer will convert id's to embeddings for us so we need to change the way we represent our word data. This time we'll just store character & phoneme ids instead of their 1-hot representations. For simplicity, we'll continue to use the 1-hot representation of phonemes for our decoders's output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "77b6604223e43dd722e6edf2e7948a7d3d152f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Word Matrix Shape:  (5333, 18)\n",
      "Embedding Phoneme Matrix Shape:  (5333, 19)\n"
     ]
    }
   ],
   "source": [
    "def dataset_for_embeddings():\n",
    "    char_seqs = []\n",
    "    phone_seqs = []\n",
    "    \n",
    "    for word,pronuns in phonetic_dict.items():\n",
    "        word_matrix = np.zeros((MAX_CHAR_SEQ_LEN))\n",
    "        for t,char in enumerate(word):\n",
    "            word_matrix[t] = char_to_id[char]\n",
    "        for pronun in pronuns:\n",
    "            pronun_matrix = np.zeros((MAX_PHONE_SEQ_LEN))\n",
    "            phones = [START_PHONE_SYM] + pronun.split() + [END_PHONE_SYM]\n",
    "            for t, phone in enumerate(phones):\n",
    "                pronun_matrix[t] = phone_to_id[phone]\n",
    "                \n",
    "            char_seqs.append(word_matrix)\n",
    "            phone_seqs.append(pronun_matrix)\n",
    "    \n",
    "    return np.array(char_seqs), np.array(phone_seqs)\n",
    "\n",
    "            \n",
    "char_emb_matrix, phone_emb_matrix = dataset_for_embeddings()        \n",
    "\n",
    "print('Embedding Word Matrix Shape: ', char_emb_matrix.shape)\n",
    "print('Embedding Phoneme Matrix Shape: ', phone_emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47c498dfcb64e1d44931b84f70cc89db4877f79b"
   },
   "source": [
    "We need to redo the train/test split. Notice that `random_state` is the same as before so the examples in each set will be the same. It also means we don't need to redo the split for our 1-hot (decoder output) phoneme matrix since we're reusing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b1c3a8b823d531478b468451b3c531b2c07eb185"
   },
   "outputs": [],
   "source": [
    "(emb_char_input_train, emb_char_input_test, \n",
    " emb_phone_input_train, emb_phone_input_test) = train_test_split(\n",
    "    char_emb_matrix, phone_emb_matrix, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5edd80b63d4cabbd7d8038a09f9ebeebb44adf56"
   },
   "source": [
    "Finally we can add the new embedding layers to our baseline model. Since they add a lot more trainable parameters to our network it's going to be easier to overfit. Let's try to avoid that by adding a few dropout layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "69212b2d94ce444d6dbf0b5a5ee09aaa66a0e4d3"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dropout, Activation\n",
    "\n",
    "def embedding_model(hidden_nodes = 256, emb_size = 256):\n",
    "    \n",
    "    # Shared Components - Encoder\n",
    "    char_inputs = Input(shape=(None,))\n",
    "    char_embedding_layer = Embedding(CHAR_TOKEN_COUNT, emb_size, input_length=MAX_CHAR_SEQ_LEN, mask_zero=True)\n",
    "    encoder = LSTM(hidden_nodes, return_state=True, recurrent_dropout=0.1)\n",
    "    \n",
    "    # Shared Components - Decoder\n",
    "    phone_inputs = Input(shape=(None,))\n",
    "    phone_embedding_layer = Embedding(PHONE_TOKEN_COUNT, emb_size, mask_zero=True)\n",
    "    decoder = LSTM(hidden_nodes, return_sequences=True, return_state=True, recurrent_dropout=0.1)\n",
    "    decoder_dense = Dense(PHONE_TOKEN_COUNT, activation='softmax')\n",
    "    \n",
    "    # Training Model\n",
    "    char_embeddings = char_embedding_layer(char_inputs)\n",
    "    char_embeddings = Activation('relu')(char_embeddings)\n",
    "    char_embeddings = Dropout(0.5)(char_embeddings)\n",
    "    _, state_h, state_c = encoder(char_embeddings)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    phone_embeddings = phone_embedding_layer(phone_inputs)\n",
    "    phone_embeddings = Activation('relu')(phone_embeddings)\n",
    "    phone_embeddings = Dropout(0.5)(phone_embeddings)\n",
    "    decoder_outputs, _, _ = decoder(phone_embeddings, initial_state=encoder_states)\n",
    "    decoder_outputs = Dropout(0.5)(decoder_outputs)\n",
    "    phone_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    training_model = Model([char_inputs, phone_inputs], phone_outputs)\n",
    "    \n",
    "    # Testing Model - Encoder\n",
    "    testing_encoder_model = Model(char_inputs, encoder_states)\n",
    "    \n",
    "    # Testing Model - Decoder\n",
    "    decoder_state_input_h = Input(shape=(hidden_nodes,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_nodes,))\n",
    "    decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    test_decoder_outputs, decoder_state_h, decoder_state_c = decoder(phone_embeddings, initial_state=decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    test_phone_outputs = decoder_dense(test_decoder_outputs)\n",
    "    \n",
    "    testing_decoder_model = Model([phone_inputs] + decoder_state_inputs, [test_phone_outputs] + decoder_states)\n",
    "    \n",
    "    return training_model, testing_encoder_model, testing_decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90ee3d76d8e89c539dd3c4e84ecf727ea2c2b59c"
   },
   "source": [
    "## Training (Embedding Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "d5e10c979610df96df9d703c2a866644d89d32d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tarun/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL_WEIGHTS = os.path.join(\n",
    "    './', 'predicting-english-pronunciations-model-weights', 'embedding_model_weights.hdf5')\n",
    "emb_training_model, emb_testing_encoder_model, emb_testing_decoder_model = embedding_model()\n",
    "if not LOAD_MODELS:\n",
    "    train(emb_training_model, EMBEDDING_MODEL_WEIGHTS, emb_char_input_train, emb_phone_input_train, phone_output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c5764dbe8ab16515ad0d4dcd290b1c03a189d7a"
   },
   "source": [
    "## Evaluation (Embedding Model)\n",
    "To evaluate our embedding model we need to add a new helper method to convert id based representations back into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "4ae7c598a24ddac542772b2cadf04c9a074b6624"
   },
   "outputs": [],
   "source": [
    "def id_vec_to_word(emb_char_seq):\n",
    "    word = ''\n",
    "    for char_id in emb_char_seq[0]:\n",
    "        char = id_to_char[char_id]\n",
    "        word += char\n",
    "    return word.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d09db9f558d34c30775fbd849769c7a35ed43e7"
   },
   "source": [
    "We'll also rewrite the prediction method to work with id representations (instead of 1-hot) before we score our new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "3b66de8ccfddccdebdf08c905cd6edc2a143b8a0"
   },
   "outputs": [],
   "source": [
    "def predict_emb(input_char_seq, encoder, decoder):\n",
    "    state_vectors = encoder.predict(input_char_seq) \n",
    "    output_phone_seq = np.array([[phone_to_id[START_PHONE_SYM]]])\n",
    "    \n",
    "    end_found = False \n",
    "    pronunciation = '' \n",
    "    while not end_found:\n",
    "        decoder_output, h, c = decoder.predict([output_phone_seq] + state_vectors)\n",
    "        \n",
    "        # Predict the phoneme with the highest probability\n",
    "        predicted_phone_idx = np.argmax(decoder_output[0, -1, :])\n",
    "        predicted_phone = id_to_phone[predicted_phone_idx]\n",
    "        \n",
    "        pronunciation += predicted_phone + ' '\n",
    "        \n",
    "        if predicted_phone == END_PHONE_SYM or len(pronunciation.split()) > MAX_PHONE_SEQ_LEN: \n",
    "            end_found = True\n",
    "        \n",
    "        # Setup inputs for next time step\n",
    "        output_phone_seq = np.array([[predicted_phone_idx]])\n",
    "        state_vectors = [h, c]\n",
    "        \n",
    "    return pronunciation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "681f2a5b53cb6fa60707f8fe055ea2947219fbf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model\n",
      "--------------------\n",
      "Syllable Accuracy: 97.0%\n",
      "Perfect Accuracy: 66.0%\n",
      "Bleu Score: 0.775\n"
     ]
    }
   ],
   "source": [
    "emb_training_model.load_weights(EMBEDDING_MODEL_WEIGHTS) # also loads weights for testing models\n",
    "syllable_acc, perfect_acc, avg_bleu_score = evaluate(\n",
    "    emb_char_input_test, emb_testing_encoder_model, emb_testing_decoder_model, id_vec_to_word, predict_emb)\n",
    "print_results('Embedding Model', syllable_acc, perfect_acc, avg_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Adding embedding layers and dropout resulted in a solid improvement on all our metrics. \n",
    "\n",
    "Let's try out our performance on some random word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NICK : [[17. 12.  6. 14.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "predict_word = 'NICK'\n",
    "predict_char_input = np.array([np.zeros(MAX_CHAR_SEQ_LEN)])\n",
    "for idx, letter in enumerate(predict_word):\n",
    "    predict_char_input[0][idx] = char_to_id[letter]\n",
    "print(predict_word, ':', predict_char_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N IH1 K'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emb(predict_char_input, emb_testing_encoder_model, emb_testing_decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1596637e86ab8bbf41279e5a352bf100273097e3"
   },
   "source": [
    "### Visualizing the Embeddings\n",
    "For fun, let's extract the embeddings the model learned and visualize them with t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "253b5d0fdb3a850a06c6e67ab72ab165db901e27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def plot_embeddings(embeddings, symbols, perplexity):\n",
    "    embeddings_in_2D = TSNE(n_components=2,perplexity=perplexity).fit_transform(embeddings)\n",
    "    embeddings_in_2D[:,0] = embeddings_in_2D[:,0] / np.max(np.abs(embeddings_in_2D[:,0]))\n",
    "    embeddings_in_2D[:,1] = embeddings_in_2D[:,1] / np.max(np.abs(embeddings_in_2D[:,1]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6,6)\n",
    "    ax.scatter(embeddings_in_2D[:,0], embeddings_in_2D[:,1],c='w')\n",
    "\n",
    "    for i, letter in enumerate(symbols):\n",
    "        ax.annotate(letter, (embeddings_in_2D[i,0],embeddings_in_2D[i,1]), fontsize=12, fontweight='bold')\n",
    "        \n",
    "        \n",
    "char_embedding = emb_training_model.layers[2].get_weights()[0]\n",
    "plot_embeddings(char_embedding, char_to_id.keys(), 5)\n",
    "\n",
    "phone_embedding = emb_training_model.layers[3].get_weights()[0]\n",
    "plot_embeddings(phone_embedding, phone_to_id.keys(), 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "963a7a00e49fe85b1b28e16a860b5d1a7c5053d1"
   },
   "source": [
    "Pretty cool. Notice how letters and phonemes with similar sounds are grouped together.\n",
    "\n",
    "Let's reset the Tensorflow graph again and keep going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b58e81d3f5bd8a3966bd7ccff8ca07789cd2982e"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "378feed8c200e09ff85cabd98f10eba1aef7c58b"
   },
   "source": [
    "# Bidirectional Encoder & Attention Decoder\n",
    "Up to this point, our RNN models have only run in one direction and the only connection between our encoder and decoder has been the 2 state variables we pass between them (from end of the encoder --> start of the decoder). For longer words & pronunciations, those state variables might not be enough to capture the entire word and the singal from the encoder has the potential to get lost. \n",
    "\n",
    "An Attention Mechanism[video,blog] is a way to avoid this problem. We'll need to make some big changes to our model's structure. We'll be using the encoder's outputs instead of it's internal state variables. This makes it easy to make the encoder bidirectional[video,blog]. Having info about the next as well as the previous characters in a word should result in better encodings at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2073ed67e49f7cacc3b3ce0641bf24e247a97748"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Multiply, Reshape, RepeatVector, Lambda, Flatten\n",
    "from keras.activations import softmax\n",
    "\n",
    "def attention_model(hidden_nodes = 256, emb_size = 256):\n",
    "    # Attention Mechanism Layers\n",
    "    attn_repeat = RepeatVector(MAX_CHAR_SEQ_LEN)\n",
    "    attn_concat = Concatenate(axis=-1)\n",
    "    attn_dense1 = Dense(128, activation=\"tanh\")\n",
    "    attn_dense2 = Dense(1, activation=\"relu\")\n",
    "    attn_softmax = Lambda(lambda x: softmax(x,axis=1))\n",
    "    attn_dot = Dot(axes = 1)\n",
    "    \n",
    "    def get_context(encoder_outputs, h_prev):\n",
    "        h_prev = attn_repeat(h_prev)\n",
    "        concat = attn_concat([encoder_outputs, h_prev])\n",
    "        e = attn_dense1(concat)\n",
    "        e = attn_dense2(e)\n",
    "        attention_weights = attn_softmax(e)\n",
    "        context = attn_dot([attention_weights, encoder_outputs])\n",
    "        return context\n",
    "    \n",
    "    # Shared Components - Encoder\n",
    "    char_inputs = Input(shape=(None,))\n",
    "    char_embedding_layer = Embedding(CHAR_TOKEN_COUNT, emb_size, input_length=MAX_CHAR_SEQ_LEN)\n",
    "    encoder = Bidirectional(LSTM(hidden_nodes, return_sequences=True, recurrent_dropout=0.2))\n",
    "    \n",
    "    # Shared Components - Decoder\n",
    "    decoder = LSTM(hidden_nodes, return_state=True, recurrent_dropout=0.2)\n",
    "    phone_embedding_layer = Embedding(PHONE_TOKEN_COUNT, emb_size)\n",
    "    embedding_reshaper = Reshape((1,emb_size,))\n",
    "    context_phone_concat = Concatenate(axis=-1)\n",
    "    context_phone_dense = Dense(hidden_nodes*3, activation=\"relu\")\n",
    "    output_layer = Dense(PHONE_TOKEN_COUNT, activation='softmax')\n",
    "    \n",
    "    # Training Model - Encoder\n",
    "    char_embeddings = char_embedding_layer(char_inputs)\n",
    "    char_embeddings = Activation('relu')(char_embeddings)\n",
    "    char_embeddings = Dropout(0.5)(char_embeddings)\n",
    "    encoder_outputs = encoder(char_embeddings)\n",
    "    \n",
    "    # Training Model - Attention Decoder\n",
    "    h0 = Input(shape=(hidden_nodes,))\n",
    "    c0 = Input(shape=(hidden_nodes,))\n",
    "    h = h0 # hidden state\n",
    "    c = c0 # cell state\n",
    "    \n",
    "    phone_inputs = []\n",
    "    phone_outputs = []\n",
    "    \n",
    "    for t in range(MAX_PHONE_SEQ_LEN):\n",
    "        phone_input = Input(shape=(None,))\n",
    "        phone_embeddings = phone_embedding_layer(phone_input)\n",
    "        phone_embeddings = Dropout(0.5)(phone_embeddings)\n",
    "        phone_embeddings = embedding_reshaper(phone_embeddings)\n",
    "        \n",
    "        context = get_context(encoder_outputs, h)\n",
    "        phone_and_context = context_phone_concat([context, phone_embeddings])\n",
    "        phone_and_context = context_phone_dense(phone_and_context)\n",
    "        \n",
    "        decoder_output, h, c = decoder(phone_and_context, initial_state = [h, c])\n",
    "        decoder_output = Dropout(0.5)(decoder_output)\n",
    "        phone_output = output_layer(decoder_output)\n",
    "        \n",
    "        phone_inputs.append(phone_input)\n",
    "        phone_outputs.append(phone_output)\n",
    "    \n",
    "    training_model = Model(inputs=[char_inputs, h0, c0] + phone_inputs, outputs=phone_outputs)\n",
    "    \n",
    "   # Testing Model - Encoder\n",
    "    testing_encoder_model = Model(char_inputs, encoder_outputs)\n",
    "\n",
    "    # Testing Model - Decoder\n",
    "    test_prev_phone_input = Input(shape=(None,))\n",
    "    test_phone_embeddings = phone_embedding_layer(test_prev_phone_input)\n",
    "    test_phone_embeddings = embedding_reshaper(test_phone_embeddings)\n",
    "    \n",
    "    test_h = Input(shape=(hidden_nodes,), name='test_h')\n",
    "    test_c = Input(shape=(hidden_nodes,), name='test_c')\n",
    "    \n",
    "    test_encoding_input = Input(shape=(MAX_CHAR_SEQ_LEN, hidden_nodes*2,))\n",
    "    test_context = get_context(test_encoding_input, test_h)\n",
    "    test_phone_and_context = Concatenate(axis=-1)([test_context, test_phone_embeddings])\n",
    "    test_phone_and_context = context_phone_dense(test_phone_and_context)\n",
    "        \n",
    "    test_seq, out_h, out_c = decoder(test_phone_and_context, initial_state = [test_h, test_c])\n",
    "    test_out = output_layer(test_seq)\n",
    "    \n",
    "    testing_decoder_model = Model([test_prev_phone_input, test_h, test_c, test_encoding_input], [test_out,out_h,out_c])\n",
    "    \n",
    "    return training_model, testing_encoder_model, testing_decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb2c0ad04be28bc610fba383a64593393d35444c"
   },
   "source": [
    "## Training (Attention Model)\n",
    "Since our model, inputs, and outputs have changed so much from our 2 previous versions we need to slightly rewrite our traing procedure. We'll still stop when we start to overfit or plateau and we'll keep the weights that achieve the best validation set loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce8d4e4fc360256d6a83e605c9aab721690ee6ee"
   },
   "outputs": [],
   "source": [
    "def train_attention(model, weights_path, validation_size=0.2, epochs=100):    \n",
    "    h0 = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    c0 = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    inputs = list(emb_phone_input_train.swapaxes(0,1))\n",
    "    outputs = list(phone_output_train.swapaxes(0,1))\n",
    "    \n",
    "    callbacks = []\n",
    "    if validation_size > 0:\n",
    "        checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, save_best_only=True)\n",
    "        stopper = EarlyStopping(monitor='val_loss',patience=3)\n",
    "        callbacks = [checkpointer, stopper]\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit([emb_char_input_train, h0, c0] + inputs, outputs,\n",
    "              batch_size=256,\n",
    "              epochs=epochs,\n",
    "              validation_split=validation_size,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    if validation_size == 0:\n",
    "        model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b77340ea8c1ed365dbaa4f06ebc41ebf49be1e3"
   },
   "outputs": [],
   "source": [
    "ATTENTION_MODEL_WEIGHTS = os.path.join(\n",
    "    './', 'predicting-english-pronunciations-model-weights', 'attention_model_weights.hdf5')\n",
    "attn_training_model, attn_testing_encoder_model, attn_testing_decoder_model = attention_model()\n",
    "if not LOAD_MODELS:\n",
    "    train_attention(attn_training_model, ATTENTION_MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c07a0cc29c571103041d135b8beb752d4f79252"
   },
   "source": [
    "## Evaluation (Attention Model)\n",
    "We also need to make some modifications to our baseline_predict method to get it working with our attention model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5328137af52032d27e8f681b39b51a5f5bdd5db4"
   },
   "outputs": [],
   "source": [
    "def predict_attention(input_char_seq, encoder, decoder):\n",
    "    encoder_outputs = encoder.predict(input_char_seq) \n",
    "\n",
    "    output_phone_seq = np.array([[phone_to_id[START_PHONE_SYM]]])\n",
    "    \n",
    "    h = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    c = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    \n",
    "    end_found = False \n",
    "    pronunciation = '' \n",
    "    while not end_found:\n",
    "        decoder_output, h, c = decoder.predict([output_phone_seq, h, c, encoder_outputs])\n",
    "        \n",
    "        # Predict the phoneme with the highest probability\n",
    "        predicted_phone_idx = np.argmax(decoder_output[0,:])\n",
    "        predicted_phone = id_to_phone[predicted_phone_idx]\n",
    "        \n",
    "        pronunciation += predicted_phone + ' '\n",
    "        \n",
    "        if predicted_phone == END_PHONE_SYM or len(pronunciation.split()) > MAX_PHONE_SEQ_LEN: \n",
    "            end_found = True\n",
    "        \n",
    "        # Setup inputs for next time step\n",
    "        output_phone_seq = np.array([[predicted_phone_idx]])\n",
    "        \n",
    "    return pronunciation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcc56b91956f52ac8b4f4c46687f13d136152bf7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Model\n",
      "--------------------\n",
      "Syllable Accuracy: 98.3%\n",
      "Perfect Accuracy: 72.8%\n",
      "Bleu Score: 0.8143\n"
     ]
    }
   ],
   "source": [
    "attn_training_model.load_weights(ATTENTION_MODEL_WEIGHTS) # also loads weights for testing models\n",
    "syllable_acc, perfect_acc, avg_bleu_score = evaluate(\n",
    "    emb_char_input_test, attn_testing_encoder_model, attn_testing_decoder_model, id_vec_to_word, predict_attention)\n",
    "print_results('Attention Model', syllable_acc, perfect_acc, avg_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd335138e248510ca6b3dc567cd874ebeed98e0a"
   },
   "source": [
    "Awesome, all that extra work appears to have paid off. Again we managed a solid improvement in all of our metrics. \n",
    "\n",
    "## Final Training and Evaluation\n",
    "Now that we know which model works best, how long we can train it before it overfits, and all our other hyper-parameters tuned, let's train one final model on all of our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80e3220fd1ef113f42201d4ac38558fb6e5d3c3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "FINAL_ATTENTION_MODEL_WEIGHTS = os.path.join(\n",
    "    './', 'predicting-english-pronunciations-model-weights', 'final_attention_model_weights.hdf5')\n",
    "attn_training_model, attn_testing_encoder_model, attn_testing_decoder_model = attention_model()\n",
    "if not LOAD_MODELS:\n",
    "    train_attention(attn_training_model, FINAL_ATTENTION_MODEL_WEIGHTS, validation_size=0.0, epochs=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b078b0a14e8f296d7c89304945d66884b236ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Attention Model\n",
      "--------------------\n",
      "Syllable Accuracy: 98.6%\n",
      "Perfect Accuracy: 75.5%\n",
      "Bleu Score: 0.8308\n"
     ]
    }
   ],
   "source": [
    "attn_training_model.load_weights(FINAL_ATTENTION_MODEL_WEIGHTS) # also loads weights for testing models\n",
    "syllable_acc, perfect_acc, avg_bleu_score = evaluate(\n",
    "    emb_char_input_test, attn_testing_encoder_model, attn_testing_decoder_model, id_vec_to_word, predict_attention)\n",
    "print_results('Final Attention Model', syllable_acc, perfect_acc, avg_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1fc555938665c586d8c740055c39aa9e4d73391e"
   },
   "source": [
    "Our numbers are looking pretty good, but we've still got 1 trick left up our sleeve...\n",
    "\n",
    "# Beamsearch\n",
    "So far, when we make predictions, we choose the phoneme with the highest probability at each timestep and then move on. But what we *really* want is the sequence of phonemes with the highest probability overall. If we make the wrong choice early on, we can easily end up with a less-than-optimal prediction and we'd never know it! One solution would be to search the entire output space and choose the best of all possible sequences. This would guarantee we find the most likely sequence (at least according to our model) but would take *forever* and involve a lot of wasted effort.\n",
    "\n",
    "Instead we can use Beamsearch[[video](https://www.coursera.org/learn/nlp-sequence-models/lecture/4EtHZ/beam-search),[blog](https://hackernoon.com/beam-search-a-search-strategy-5d92fb7817f)] which sort of falls between the two extremes. At each timestep, we keep the *k* most likely sequences and the move on. Increasing the value of *k* means we're more likely to find the optimal sequence but increases the search time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a829e3146d7af2cb3a4a4aa97a90a0ce62a21cd"
   },
   "outputs": [],
   "source": [
    "def predict_beamsearch(input_char_seq, encoder, decoder, k=4):\n",
    "    a = encoder.predict(input_char_seq) \n",
    "    \n",
    "    s = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    c = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    \n",
    "    all_seqs = []\n",
    "    all_seq_scores = []\n",
    "    \n",
    "    live_seqs = [[phone_to_id[START_PHONE_SYM]]]\n",
    "    live_scores = [0]\n",
    "    live_states = [[s,c]]\n",
    "\n",
    "    while len(live_seqs) > 0: \n",
    "        new_live_seqs = [] \n",
    "        new_live_scores = [] \n",
    "        new_live_states = []\n",
    "        \n",
    "        for sidx,seq in enumerate(live_seqs):\n",
    "            target_seq = np.array([[seq[-1]]])\n",
    "            output_token_probs, s, c = decoder.predict([target_seq] + live_states[sidx] + [a])\n",
    "            \n",
    "            best_token_indicies = output_token_probs[0,:].argsort()[-k:]\n",
    "\n",
    "            for token_index in best_token_indicies:\n",
    "                new_seq = seq + [token_index]\n",
    "                prob = output_token_probs[0,:][token_index]\n",
    "                new_seq_score = live_scores[sidx] - np.log(prob)\n",
    "                if id_to_phone[token_index] == END_PHONE_SYM or len(new_seq) > MAX_PHONE_SEQ_LEN:\n",
    "                    all_seqs.append(new_seq) \n",
    "                    all_seq_scores.append(new_seq_score) \n",
    "                    continue\n",
    "                new_live_seqs.append(new_seq)\n",
    "                new_live_scores.append(new_seq_score)\n",
    "                new_live_states.append([s, c])\n",
    "                \n",
    "        while len(new_live_scores) > k:\n",
    "            worst_seq_score_idx = np.array(new_live_scores).argsort()[-1] \n",
    "            del new_live_seqs[worst_seq_score_idx]\n",
    "            del new_live_scores[worst_seq_score_idx]\n",
    "            del new_live_states[worst_seq_score_idx]\n",
    "            \n",
    "        live_seqs = new_live_seqs\n",
    "        live_scores = new_live_scores\n",
    "        live_states = new_live_states\n",
    "        \n",
    "    best_idx = np.argmin(all_seq_scores)\n",
    "    score = all_seq_scores[best_idx]\n",
    "    \n",
    "    pronunciation = ''\n",
    "    for i in all_seqs[best_idx]:\n",
    "        pronunciation += id_to_phone[i] + ' '\n",
    "    \n",
    "    return pronunciation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a1733f64c0cde7db12ff07a7302e8123cb0b2d4"
   },
   "outputs": [],
   "source": [
    "syllable_acc, perfect_acc, avg_bleu_score = evaluate(\n",
    "    emb_char_input_test, attn_testing_encoder_model, attn_testing_decoder_model, id_vec_to_word, predict_beamsearch)\n",
    "print_results('Final Attention Model + Beamsearch', syllable_acc, perfect_acc, avg_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3973aae34b579d42272eb5f087122ec712e1c09c",
    "collapsed": true
   },
   "source": [
    "# Results\n",
    "Here are the final results I obtained locally on the complete test set (~20k examples):\n",
    "\n",
    "| Model        | Baseline | Embedding | Attention | Final Attention | Final Attention + Beamsearch |\n",
    "|--------------|----------|-----------|-----------|-----------------|------------------------------|\n",
    "| Syllable Acc | 91.3        |   96.0        |   96.8        |     97.1            | **98.1**                             |\n",
    "| Perfect Acc  | 51.9         |    61.5       |    66.6      |     67.6            |   **75.4**                           |\n",
    "| Bleu Score   | 0.654         |   0.734        |     0.770      |      0.778           | **0.829**                           |\n",
    "\n",
    "English is a weird language. Even native speakers mis-pronounce words that are new to them. Pronunciation rules are complicated and sometimes don't make any sense. Just check out [this video](https://www.youtube.com/watch?v=1edPxKqiptw) for a few examples. 75.4% accuracy might not seem that high, but all things considered, I think it's a respectable score. \n",
    "\n",
    " ## Sampling Incorrect Predictions\n",
    " Let's take a look at some of the words our model is getting wrong:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05c3165d086ab6c8027a57cad16c4ea442b65e10"
   },
   "outputs": [],
   "source": [
    "def display_wrong_predictions(sample_count, word_decoder, encoder, decoder):\n",
    "    found = 0\n",
    "    while found < sample_count:\n",
    "        sample_index = random.sample(range(TEST_EXAMPLE_COUNT), 1)[0]\n",
    "        example_char_seq = emb_char_input_test[sample_index:sample_index+1]\n",
    "        predicted_pronun = predict_attention(example_char_seq, encoder, decoder)\n",
    "        example_word = word_decoder(example_char_seq)\n",
    "        pred_is_correct = is_correct(example_word,predicted_pronun)\n",
    "        if not pred_is_correct:\n",
    "            found += 1\n",
    "            print('❌ ', example_word,'-->',predicted_pronun)\n",
    "            \n",
    "display_wrong_predictions(20, id_vec_to_word, attn_testing_encoder_model, attn_testing_decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c973e8c41b8e4ee7c8c7fbb18085f769bb5104b"
   },
   "source": [
    "## Ideas for Further Improvement\n",
    "* **Using 2 seperate models** Model 1 only predicts the phonemes while model 2 adds the stress markers (numbers) in the appropriate places. Knowing the length of the final sequence means our second model's decoder could easily be bidirectional. Knowing about past *and future* vowel sounds seems likely to improve our stress marker perdictions.\n",
    "* **More hyper-parameter tuning** we really didn't spend much time tweaking our hyper-parameters. It should be easy to find some better values and inprove our scores.\n",
    "* **Larger Model** adding another recurrent layer to our encoder and/or decoder or inserting some 1D convolutional layers could be worth trying.\n",
    "* **More data!** Finding our creating a dataset with more names, places and slang should help.\n",
    "\n",
    "## Advanced Techniques\n",
    "This is by no means a state-of-the-art approach to the pronunciation problem. If you're interested in better solutions make sure to check out some of these papers:\n",
    "* [Predicting Pronunciations with Syllabification and Stress with Recurrent Neural Networks](https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1419.PDF) by Daan van Esch, Mason Chua, Kanishka Rao\n",
    "* [Text-To-Phoneme Mapping Using Neural Networks](https://tutcris.tut.fi/portal/files/2313141/bilcub.pdf) by Enikö Beatrice Bilcu\n",
    "* [GRAPHEME-TO-PHONEME CONVERSION USING LONG SHORT-TERM MEMORY RECURRENT NEURAL NETWORKS](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43264.pdf) by Kanishka Rao, Fuchun Peng, Has¸im Sak, Franc¸oise Beaufays\n",
    "* [Joint-Sequence Models for Grapheme-to-PhonemeConversion](https://hal.archives-ouvertes.fr/hal-00499203/document) by Maximilian Bisani, Hermann Ney\n",
    "\n",
    "## Acknowledgements\n",
    "A lot of the ideas applied here came from Andrew Ng's super-awesome [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning).\n",
    "I used the [Keras Examples](https://github.com/keras-team/keras/tree/master/examples) on Github as a reference for the baseline model. \n",
    "I also borrowed some ideas from this [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) for the attention model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
