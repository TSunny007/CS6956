{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport collections\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['warandpeace40', 'glove-global-vectors-for-word-representation']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\n\nlower_case = True\n\nif lower_case:\n    train_file = '../input/warandpeace40/war-and-peace-trimmed-1-12.txt'\n    test_file = '../input/warandpeace40/war-and-peace-trimmed-13-15.txt'\nelse:\n    train_file = '../input/warandpeace40/war-and-peace-trimmed-1-12_original.txt'\n    test_file = '../input/warandpeace40/war-and-peace-trimmed-13-15_original.txt'\n    \nwith open (train_file) as f:\n    counter = collections.Counter([token for line in f for token in line.split()])\n    counts = counter.most_common()\n    rare = set()\n    train_words = set()\n    for i in range(len(counts)-1, -1, -1):\n        if counts[i][1] == 1:\n            rare.add(counts[i][0])\n        else:  \n            train_words.add(counts[i][0])\n            \ntrain_words.add('<na>')\ntrain_words.add('<sos>')\ntrain_words.add('<eos>')\ntrain_words.add('<unk>')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(train_file) as f:      \n    train_data = np.array([ '<sos> ' + ' '.join([token if token not in rare else '<unk>' for token in line.split()]) + ' <eos>' for line in f])","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[0:5]","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"array(['<sos> book one : 1805 <eos>', '<sos> chapter i <eos>',\n       '<sos> \" well , prince , so genoa and <unk> are now just family estates of the <unk> . <eos>',\n       \"<sos> but i warn you , if you do n't tell me that this means war , if you still try to defend the <unk> and horrors <unk> by that antichrist <eos>\",\n       \"<sos> - i really believe he is antichrist - i will have nothing more to do with you and you are no longer my friend , no longer my ' faithful slave , ' as you call yourself ! <eos>\"],\n      dtype='<U254')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(test_file) as f:\n    test_data = np.array([ '<sos> ' + ' '.join([token if token in train_words else '<unk>' for token in line.split()]) + ' <eos>' for line in f])\ntest_words = {word for sentence in test_data for word in sentence.split()}","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1: Report the number of unique tokens in each split of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('unique words in train: {}, test: {}'.format(len(train_words), len(test_words)))","execution_count":6,"outputs":[{"output_type":"stream","text":"unique words in train: 8473, test: 4645\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Question 2: Read the following paper:\n`Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. ”Recurrent neural\nnetwork regularization.” arXiv preprint arXiv:1409.2329 (2014)`\n\nExplain briefly (about 1-2 paragraphs at most) the motivation of the paper and its\ncontribution."},{"metadata":{},"cell_type":"markdown","source":"```\nThe Recurrent Neural Network sequence model achieves state of the art performance on important tasks that include language modeling Mikolov (2012), speech recognition Graves et al. (2013), and machine translation Kalchbrenner & Blunsom (2013). Most deep learning models require usage of regularization to genneralize well and to not overfit on data, and this problem effects RNN based models. Dropout, the most popular form of regularization in neural networks does not seem to work as well with RNNs because dropping sequential information can stack up over time stamps and greatly damage the hidden state when applied on recurrent connections.\n\nThis paper suggests using dropout in deep RNN blocks by only using dropout in the stacked connections and not using dropout on the recurrent connections. As a result, the flow of information is only effected L+1 times given L stacked layers. By not using dropout on the recurrent connections, the LSTM can benefit from dropout regularization without sacrificing its valuable memorization ability.\n```"},{"metadata":{},"cell_type":"markdown","source":"# Question 3: Implement an LSTM language model (LOWERCASE):"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Input, TimeDistributed, Activation\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \nimport keras.backend as K\nimport keras.losses as losses\nfrom keras.models import load_model","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary_size = len(train_words)\n\n\nvocabulary = dict(zip(sorted(train_words), np.arange(vocabulary_size)+1))\nvocabulary['<na>'] = 0\nvocabulary_size += 1 # because we haven't encountered <na> yet\nreverse_vocabulary = dict(zip(vocabulary.values(), vocabulary.keys()))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = np.array([np.array([vocabulary[token] for token in sentence.split()]) for sentence in train_data])\ntest = np.array([np.array([vocabulary[token] for token in sentence.split()]) for sentence in test_data])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[3])\nprint(test[3])","execution_count":10,"outputs":[{"output_type":"stream","text":"[  49 1063 3702 8169 8452   16 3717 8452 2223 4858 7476 4586 7518 7561\n 4592 8159   16 3717 8452 7153 7780 7632 1915 7521   50  317 3620   50\n 1075 7518  355   47]\n[  49 7536 4012   16  317 1109  691   16 4958 1179 5076  311 3569 2595\n 2631 7521 5107 1179 5076  249 1181   19   47]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_padded_sequences(input_sequences, max_len=None):\n    lengths = np.array([len(x) for x in input_sequences])\n    if max_len:\n        max_sequence_len = max_len\n    else:\n        max_sequence_len = np.max(lengths)\n    \n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='post'))\n    empty_col = np.zeros(input_sequences.shape[0])[...,None]\n    \n    X = input_sequences.copy()\n    X[np.arange(input_sequences.shape[0]), lengths-1] = 0\n    \n    Y = input_sequences.copy()[np.arange(input_sequences.shape[0]),1:]\n    Y = np.append(Y, empty_col, 1)\n    Y = Y.astype(int)\n    \n    return X, Y, lengths, max_sequence_len\n\nX, Y, lengths, max_sequence_len = generate_padded_sequences(train)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in range(3):\n    print('input: ', X[n, 0:lengths[n]-1])\n    print('output: ', Y[n, 0:lengths[n]-1])\n    print()","execution_count":12,"outputs":[{"output_type":"stream","text":"input:  [  49  911 5107   45   28]\noutput:  [ 911 5107   45   28   47]\n\ninput:  [  49 1232 3702]\noutput:  [1232 3702   47]\n\ninput:  [  49    2 8229   16 5673   16 6905 3179  317   50  426 5005 4082 2752\n 2581 5076 7521   50   19]\noutput:  [   2 8229   16 5673   16 6905 3179  317   50  426 5005 4082 2752 2581\n 5076 7521   50   19   47]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocabulary_size, 100))\nfound = 0\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        if word in vocabulary:\n            found += 1\n            embedding_matrix[vocabulary[word]] = np.asarray(values[1:], dtype='float32')\nprint(f'found {found} word entries')","execution_count":13,"outputs":[{"output_type":"stream","text":"found 7987 word entries\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary_size","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"8474"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sparse_loss(yTrue,yPred):\n    return K.sparse_categorical_crossentropy(yTrue,yPred,from_logits=True)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":24,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(vocab_size, max_length, embedding_matrix, keep_prob=0.5):\n    model = Sequential()\n    # Add Input Embedding Layer\n    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length))\n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(200, dropout=keep_prob, return_sequences=True))\n    # Add Hidden Layer 2 - LSTM Layer\n    model.add(LSTM(200, dropout=keep_prob, return_sequences=True))\n    # model.add(LSTM(200, dropout= 0.5, return_sequences=True))\n    \n    # Add Output Layer\n    model.add(TimeDistributed(Dense(vocabulary_size, activation='linear')))\n    model.compile(loss=sparse_loss,  optimizer='adam')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Have been using incremental epoch training`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# if 'model.h5' in os.listdir(\"../input/warandpeace40/\"):\n#    losses.sparse_loss = sparse_loss\n#    model = load_model(\"../input/warandpeace40/model.h5\")\n#    model.summary()\nmodel = create_model(vocabulary_size, max_sequence_len, embedding_matrix)\nmodel.summary()\nhistory = model.fit(X, Y, epochs=50)","execution_count":53,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 42, 100)           847400    \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 42, 200)           240800    \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 42, 8474)          1703274   \n=================================================================\nTotal params: 2,791,474\nTrainable params: 2,791,474\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"Error when checking target: expected time_distributed_1 to have 3 dimensions, but got array with shape (24112, 42)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-aad06a4091d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_1 to have 3 dimensions, but got array with shape (24112, 42)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X[0:100])\nnp.argmax(pred, axis=2).shape","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"(100, 42)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in np.argmax(pred, axis=2):\n    print(' '.join([reverse_vocabulary[token] for token in col if token != 0 ]))","execution_count":28,"outputs":[{"output_type":"stream","text":"\" the of 1812 <eos>\n\" iii <eos>\n\" i , what , \" genoa , <unk> , <unk> , , , , the vistula of <eos>\n\" the am you , \" i know n't know me that i is of is \" i know , to get the army of the of of the antichrist <eos>\n\" i'll am , it is not - i am tell been to than him . you . have will not longer . fault . \" longer , father . slave . \" i you please him . \"\n\" the is you think ? <eos>\n\" am you am been you , you down . have me to . same . <eos> <eos>\n\" was evident the , and , and the rost'ovs was a same - known grandee p'avlovna sch'erer , who , the . the of the rost'ovs . f\"edorovna . <eos>\n\" the words he had her andrew and , who footman of the - , <unk> . and had not same time the at the . . <eos>\n\" p'avlovna was come come moment , her time , <eos>\n\" was not and if had to and , her grippe , grippe <unk> <unk> a <unk> <unk> of the p'avlovna . and to to the <unk> of <eos>\n\" the invitations she exception , and , the , and the by the smile , liveried footman , had , and out far : <eos>\n\" i ! <eos>\n\" is calamity ! ! \" <eos>\n\" the count , who knowing the same abashed , the movement of <eos>\n\" was not been the and a hour overcoat uniform , and breeches , and a , and with a on his face . went little highness of his face face . <eos>\n\" was to the state tone , the he troops had <unk> , of he that and the a same animation patronizing intonation of to his man . his . had been up . the . the the . <eos>\n\" was to to the mikh'aylovna 's and her , , and her her mother hand hills and , and the eyes , and the . her in the sofa . <eos>\n\" i , the the \" , , \" me ! you are , <eos>\n\" off way 's hand ! the , \" said the , looking his eyes . \" his <unk> and <unk> his . his he to <unk> his . not discerned . <eos>\n\" i you be a ? ? ? ? \"\n\" you be killed , the , a men he can been <unk> of \" <eos> the p'avlovna . <eos>\n\" i know not , same army ? \" am you \" <eos>\n\" i what devil is once niemen club , birthday \"\n\" , the . <eos>\n\" am have it the hour of , \" said the countess . <eos>\n\" i dear is a to me , the her a . <eos> <eos>\n\" i am , , , , been <unk> , <eos>\n\" am you the men and <unk> are not wearisome . <eos> <eos>\n\" i n't you me \"\n\" , what i is happened a ? ? ? ? ? <eos>\n\" know , , <eos> <eos>\n\" i is you be ? ? ? \" <eos>\n\" the count , the tone perspiration hesitating voice . <eos>\n\" i is happened a ? \"\n\" were been to the is been the <unk> . and i will it it are not to defend . . <eos> <eos>\n\" andrew , was of and and a old of the <unk> . . <eos>\n\" p'avlovna was , the contrary , and the brother - , and her her , <unk> , <eos>\n\" the silent hour , been <unk> , , and the the the the she had not know ashamed a . but was more and her to to be her whole of the who had that . <eos>\n\" the middle of the <unk> he the sides , p'avlovna 's into of <eos>\n\" i , yes n't you to me ! me , <eos>\n\" i am n't know , , \" i is will been to \" i not know to \" the , <eos>\n\" was so her , \"\n\" is is be me , <eos>\n\" men sovereign recognizes the vocation vocation and the not <unk> . me . <eos>\n\" 's what matter thing i am been in the <eos>\n\" men - <unk> ! is been be the army of of the , and the is a kind , <unk> . the knows be have the . <eos>\n\" was not his godson and the him <unk> of his and \" is been <unk> than than ever . the world . the murderer . the . <eos>\n\" 'll , be the emperor of the <unk> . of <eos>\n\" he i am you to \" not have on the <eos> <eos>\n\" is the father and <unk> be be have not exist . <unk> . 's presence . the . <eos>\n\" was been to be her , <eos>\n\" was to say her but she seeks , and of motive of her brotherhood . <eos>\n\" is ? not get up <eos>\n\" of <eos>\n\" old club been yet that that not be . whole - mindedness , the fatherland . has to to me . \" i i the whole - the . <eos>\n\" he is you done ? <eos>\n\" , <eos>\n\" he is nicholas are been to have be have the \"\n\" is been been that the is not , \" i is the is powerless , the , <eos>\n\" he am n't know it word of i , , \" i , the <eos>\n\" is man army of <unk> a <unk> . <eos>\n\" am been in to the 's will fatherland destiny of the fatherland monarch . <eos>\n\" was not me to \" <eos>\n\" was screamed . and , her . impetuosity . <eos>\n\" i am i \" said the count . a smile . and i is you do to sent to of the brotherhood wintzingerode , will have been the army of prussia . knight . the . <eos>\n\" know not happy . <eos>\n\" you see me ? little of the ? \" <eos>\n\" i the minute , <eos>\n\" was not of the same , <unk> , \" fatherland - , <eos>\n\" he the same morio . <eos>\n\" n't know ? ? ? ? \"\n\" was been degraded by the army 's <eos>\n\" he come the \" <eos>\n\" i am not a , the you , \" said the countess . <eos>\n\" i\n\" old who the the , a <unk> man , <eos> <eos>\n\" andrew , to say a question to the father 's and he , not to the same empress . dm'itrievna . the her . the first . <eos>\n\" p'avlovna was screamed the eyes . her that she she had her of . come moment to criticize her she whole had to to to to her <eos>\n\" i funke has been recommended to the bravest empress , the son , \" she the the said , \" the tone - mournful voice . <eos>\n\" soon had the rost'ovs , who p'avlovna sch'erer arrival was assumed a invitation of a excitement sincere devotion to devotion . with her . and she was to day was was . . . . <eos>\n\" was , she brother was been to come her funke , . . and the began face became her her her . <eos>\n\" old was a , , at at <eos>\n\" i , the honor , \"\n\" n't know ? ? i father ? ? ? ? been in by ? ? <eos>\n\" were that is not , , <eos> <eos>\n\" old was his the the eyes and <unk> , <eos>\n\" is been ? me ? days men ? ? <eos>\n\" am n't know to me . \" excellency , \"\n\" am n't know a ,\n\" i he said , a whisper of that her importance , <unk> her head . <eos> <eos>\n\" days men men , <eos>\n\" he , 'll me to than anyone else \" i i are n't know to do done . <eos> <eos>\n\" he was . . smile . <eos>\n\" i am n't tell you , \" said the countess . <eos>\n\" i ! have been the 'm of army of the , <eos> <eos>\n\" i n't you , i 'll , you a t^ete-`a , of you , \"\n\" n't know ? am afraid ? you father ? ? <eos>\n\" the , <eos> he the voice assumed a <unk> smile , \" \" i is saying , the . . sake the . not . <eos> <eos>\n\" old was the , and he did at her with . and her moment . <eos>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test, Y_test, lengths_test, _ = generate_padded_sequences(test)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X[0])\nprint(np.argmax(pred[0], axis=1))\nprint(Y[0])","execution_count":31,"outputs":[{"output_type":"stream","text":"[  49  911 5107   45   28    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n[   2 7521   16   35   47    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n[ 911 5107   45   28   47    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax(s):\n    shift_scores = s - np.max(s)\n    return np.exp(shift_scores)/np.sum(np.exp(shift_scores))","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perplexity(pred, labels, sent_lengths):\n    perp = []\n    for idx, example in enumerate(pred):\n        pp = 0\n        for word_idx in range(sent_lengths[idx]-2):\n            pp += np.log(softmax(pred[idx, word_idx, :])[labels[idx][word_idx]])\n        perp.append(-pp/(sent_lengths[idx]-1))\n    return perp","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perplexity on test"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.exp(np.mean(perplexity(pred, Y_test, lengths_test))))","execution_count":34,"outputs":[{"output_type":"stream","text":"72.09152590741388\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Question 5: Example text:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    sent = np.zeros(X.shape[1])\n    sent[0] = vocabulary['<sos>']\n    pick = None\n    idx = 1\n    while (pick != vocabulary['<eos>']) and idx < X.shape[1]:\n        model_pred = model.predict(np.array([sos]))\n        model_pred = model_pred[0] # get the first example in \"batch\"\n        model_pred = model_pred[0] # get the first RNN output\n        pred = softmax(model_pred)\n        pick = np.random.choice(vocabulary_size, p=pred) # choosing from this distribution for our first word\n        sent[idx] = pick\n        idx += 1\n\n    print(' '.join([reverse_vocabulary[word] for word in sent]))\n    print()","execution_count":55,"outputs":[{"output_type":"stream","text":"<sos> so le meeting \" but chapter \" in he to \" i rost'ov \" his \" \" \" prince what father i \" kur'agin \" \" only \" ( what \" it this i napoleon at d'olokhov soon \" \" \"\n\n<sos> balash\"ev good there \" others he \" what napoleon p'etya everything \" why everybody \" \" \" pierre \" \" this i he the she prince how he \" \" the it after \" \" ah the \" he \" she\n\n<sos> \" eh this \" nat'asha at there \" he nicholas i prince \" yet he \" <unk> the \" without he \" it pfuel i he m'arya he it there while the suv'orov there <unk> the \" \" at next the\n\n<sos> really and \" \" some a he \" they the ... if and \" \" the yes the \" he she this alp'atych i \" evidently but but chapter she he \" there he \" thought oh \" they nat'asha un\n\n<sos> this i do \" \" at she she \" \" tim'okhin i but my occasionally the - \" we he chapter and you \" den'isov a go but it all \" \" having at his \" \" the an why mind\n\n<sos> \" \" nat'asha some she let \" she the the if a well he a on the but since a \" \" \" \" mimi bazd'eev \" let the \" and \" nicholas \" nat'asha both and \" again \" the\n\n<sos> a s'onya \" for \" \" \" bonaparte \" the and morel i do i a chapter who and and ah her \" he what \" but but he the a \" \" without several nicholas this what all those that\n\n<sos> the in he at screamed he that and am \" this * he and \" he fine well he \" it \" it \" she \" god and \" i he prince prince \" on his on and and several in\n\n<sos> that since a \" \" no pierre \" they it call the prince \" look in \" her <unk> the \" anatole he \" the oh oh \" another alone now you there \" pierre they she he prince i i\n\n<sos> * and tim'okhin \" \" he \" the \" four \" order the the i it * on \" the seeing \" \" why having \" \" he chapter i he nat'asha oh \" \" moscow \" i mak'ar rost'ov \"\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Question 6: Report the perplexity of both language models on the following sentences from The Tale of Two Cities:"},{"metadata":{},"cell_type":"markdown","source":"* It was the best of times, it was the worst of times . \n* It was the age of wisdom, it was the age of foolishness.\n* It was the epoch of belief, it was the epoch of incredulity. \n* It was the season of Light, it was the season of Darkness.\n* It was the spring of hope, it was the winter of despair. \n* We had everything before us, we had nothing before us .\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data = ['It was the best of times , it was the worst of times', \n            'It was the age of wisdom , it was the age of foolishness',\n            'It was the epoch of belief , it was the epoch of incredulity', \n            'It was the season of Light it was the season of Darkness',\n            'It was the spring of hope , it was the winter of despair', \n            'We had everything before us , we had nothing before us']\nval_data = ['<sos> ' + sent + ' <eos>' for sent in val_data]\nif lower_case:\n    val_data = [sent.lower() for sent in val_data]","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val = np.array([np.array([vocabulary[token] if token in vocabulary else vocabulary['<unk>'] for token in sentence.split()]) for sentence in val_data])\nval","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"array([array([  49, 4018, 8175, 7521,  789, 5076, 7620,   16, 4018, 8175, 7521,\n       8375, 5076, 7620,   47]),\n       array([  49, 4018, 8175, 7521,  206, 5076, 8317,   16, 4018, 8175, 7521,\n        206, 5076,   50,   47]),\n       array([  49, 4018, 8175, 7521, 2549, 5076,  753,   16, 4018, 8175, 7521,\n       2549, 5076,   50,   47]),\n       array([  49, 4018, 8175, 7521,   50, 5076, 4317, 4018, 8175, 7521,   50,\n       5076, 1849,   47]),\n       array([  49, 4018, 8175, 7521, 7054, 5076, 3607,   16, 4018, 8175, 7521,\n       8311, 5076, 2004,   47]),\n       array([  49, 8198, 3391, 2603,  732, 7965,   16, 8198, 3391, 4990,  732,\n       7965,   47])], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val, Y_val, lengths_val, _ = generate_padded_sequences(val, X.shape[1])","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_val[0])\nprint(Y_val[0])","execution_count":59,"outputs":[{"output_type":"stream","text":"[  49 4018 8175 7521  789 5076 7620   16 4018 8175 7521 8375 5076 7620\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n[4018 8175 7521  789 5076 7620   16 4018 8175 7521 8375 5076 7620   47\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val = model.predict(X_val)","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## perplexities:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.exp(perplexity(pred_val, Y_val, lengths_val)))","execution_count":61,"outputs":[{"output_type":"stream","text":"[39.69536812 61.53427454 75.07962459 35.0528755  59.86974412 65.49199422]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":62,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}