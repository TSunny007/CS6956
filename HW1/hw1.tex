% ===============================================
% MATH 34: Multivariable calculus           Spring 2019
% hw_template.tex
% ===============================================

% -------------------------------------------------------------------------
% You can ignore this preamble. Go on
% down to the section that says "START HERE" 
% -------------------------------------------------------------------------

\documentclass{article}

\usepackage[margin=1.5in]{geometry} % Please keep the margins at 1.5 so that there is space for grader comments.
\usepackage{amsmath,amsthm,amssymb,hyperref}

\newcommand{\R}{\mathbf{R}}  
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Q}{\mathbf{Q}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{claim}[2][Claim]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

\begin{document}

\large % please keep the text at this size for ease of reading.

% ------------------------------------------ %
%                 START HERE             %
% ------------------------------------------ %

\begin{center}
{\Large CS6956 - HW1 - Tarun Sunkaraneni} % Replace "Author's Name" with your name
\end{center}
\vspace{0.05in}

% -----------------------------------------------------
% The "enumerate" environment allows for automatic problem numbering.
% To make the number for the next problem, type " \item ". 
% To make sub-problems such as (a), (b), etc., use an "enumerate" within an "enumerate."
% -----------------------------------------------------

\begin{enumerate}

\item In class, we saw that word2vec employed the idea of negative sampling to estimate its parameters. In this question, you will go into the details of this technique. In particular, you will explore the connection between negative sampling and a different estimation technique called noise contrastive estimation.

\begin{enumerate}
\item Explain why techniques such as noise contrastive estimation or negative sampling are needed for training word embeddings? Is the problem still relevant given modern GPU based architectures?\\~\\
Whenever we want to evaluate a probability for a word given a context, we will have to deal with summing up the probabilities of every word in the vocabulary given that particular context. This is intractable because vocabulary size is in the order of millions. Even if we were hypothetically  able to to train such a model using insane compute, forward pass on such a model would take a lot of time, and usually that part of the process is required to be immediate. Therefore, it is a problem to deal with any sensible GPU or CPU architectures.\\
\item In section 3, the note mentions that negative sampling can be seen as a special case of noise contrastive estimation if certain conditions are met. Show that this argument is correct by working out the math.\\
The paper gives the formula for NCE to be:
�� $$p(D=0| w ,c) = \frac{k * q(w)}{u_\theta(w,c) + k * q(w)} $$
$$p(D=1| w ,c) = \frac{u_\theta(w,c)}{u_\theta(w,c) + k * q(w)} $$
If we plug in $k = |V|$ and $q(w) = \frac{1}{|V|}$, we get that $ k * q(w) = 1$, at which point we can derive the negative sampling form:
$$p(D=0| w ,c) = \frac{1}{u_\theta(w,c) + 1} $$
$$p(D=1| w ,c) = \frac{u_\theta(w,c)}{u_\theta(w,c) + 1} $$
\end{enumerate}



% -----------------------------------------------------
% Second problem
% -----------------------------------------------------
\item In class, we saw that word embeddings are dense, low-dimensional embed- dings of text. An alternative approach could be to use what are sometimes called “traditional NLP features”, which are sparse and high dimensional. A natural question could be about how these two methods compare with each other.
\setlength{\parindent}{10ex}\par
There are two big contributions this paper presents. The main contribution shows that using explicit context representations can yield comparable results to a neural network representation when tested for certain criteria. The second big contribution of this paper is the slightly tweaked objective function which outperforms the state-of- the-art at recovering relational similarities \textit{under both} neural and bag of context representations. The authors argue that this means the patterns which neural models have been able to "learn" still can be captured in the bag of contexts setting. \par
One thing that is different than most other papers i've read is this authors' effort to help a reader replicate the experiments ran for the paper. From explaining Word2Vec parameters to pruning criteria for infrequent words, the authors show a level of transparency and implicitly urge readers to replicate and verify the results for themselves. The paper also mentions the work of Mikolov numerous times, and compares its findings to Mikolov's findings throughout the paper.  \par
It would be interesting to see what causes "default word" answers, and to check whether those answers hold across \textit{both} the neural and bag of context embeddings in order to gain more insights about what could be causing it. I would also like to look into building an effective ensemble out of both the embeddings, which can attain a proposed maximum accuracy in the 70\%+. \par
The experiments reported in the paper are clearly stated and their results are concise. In addition, every experiment performed seems to directly build up to a overarching idea that tweaking the objective function can give substantial gains to existing work. I appreciate that they replicated  the Mikolov experiment with the only change being the proposed 3CosMul objective brought up the evaluation score significantly. \par
The paper uses an abundant and standard dataset for both training and evaluation, and the absolute percentage improvement of 20\% is pretty remarkable. Given that the authors were able to identify what categories each type of embedding did better on and were able to generalize each's strengths is impressive, and gives hope that we can use context as humans understand (neighborhood of words) to achieve good results with a statistical embedding model. \par
The results of this paper are for the most part persuasive because two large datasets were used for evaluation, and the authors took the time to implement existing works to document some baseline percentages. I am \textbf{wary} that they stopped using SemEval to evaluate their \textit{key} contributions, so I cannot help but believe that that the 3CosMul objective is not the perfect word embedding objective function. However, I am satisfied to hear that the \textit{bag of contexts} embedding procedure does have the percentages to serve as a good baseline for tasks at hand.

\end{enumerate}

% ---------------------------------------------------
% Anything after the \end{document} will be ignored by the typesetting.
% ----------------------------------------------------

\end{document}
